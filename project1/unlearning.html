
<!DOCTYPE html>
<html>
    <head>
        <title>Machine Unlearning</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link
            rel="icon"
            href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ§© </text></svg>"
        />
        <link rel="stylesheet" href="base.css" type="text/css" />

    </head>
    <body>
        <header>
            <nav>
            <ul>
                <li><a href="index.html">index</a></li>
                <li><a href="math.html">math as a game</a></li>
                <li><a href="unlearning.html">machines that unlearn</a></li>
                <li><a href="zines.html">zines for losers</a></li>
            </ul>
        </nav>
        </header>
        <h1><span>Machine Unlearning</span></h1>
<p>
        Machine learning predicts the future based on data collected from the past. Machine unlearning attempts to approximate the effect of retraining a model from scratch without the "unlearned" data.

        Many papers about machine unlearning are written from the perspective of those with access to a model - machine learning engineers at a tech company, for example - who have received requests to remove training data from a machine learning system.
<br><br>

        Retraining a model is computationally expensive. A machine learning engineer can attempt to remedy the problem by removing the problematic data from the database containing training data, but without retraining, the model's predictions still rely on the problematic data. This is the problem machine unlearning papers aim to solve.

        I am interested in machine unlearning from a privacy standpoint. What if I want to remove pictures of my face from a facial recognition model? What can I do?
<br><br>

        As an undergraduate, I wondered whether I was the first person to consider doing machine unlearning research using data poisoning. I never came across this idea in the literature.

        I think machine learning - and artificial intelligence research more broadly - could be so beautiful. In 2021, I was excited to research natural language processing as an undergraduate. In 2025, it felt like I woke up in the worst alternate timeline possible.
<br><br>

        In 2021, I was still so hopeful that everything would turn out well. I still believed that AI would be funded modestly, governed by those most impacted, and downsized to address our planet's pressing environmental issues. I have learned to unlearn this optimism.

</p>
<footer>
    <nav>
    <ul>
        <li><a href="index.html">index</a></li>
        <li><a href="math.html">math as a game</a></li>
        <li><a href="unlearning.html">machines that unlearn</a></li>
        <li><a href="zines.html">zines for losers</a></li>
    </ul>
</nav>
</footer>
</html>
